{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')) == 0: \n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(modelName)\n",
    "model = TFBertForSequenceClassification.from_pretrained(modelName, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('trainingData2.csv')\n",
    "df['Text'] = df['Text'].str.lower()\n",
    "text = df['Text'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "text_train, text_temp, labels_train, labels_temp = train_test_split(text, labels, test_size=0.2, random_state=7)\n",
    "text_val, text_test, labels_val, labels_test = train_test_split(text_temp, labels_temp, test_size=0.5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "steps = len(text_train)//batchSize\n",
    "val_steps = len(text_val)//batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encodings = tokenizer(text, truncation=False)\n",
    "seq_len = []\n",
    "for encoding in all_encodings[\"input_ids\"]:\n",
    "    seq_len.append(len(encoding))\n",
    "max_length = max(seq_len) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(text_train, truncation=True, padding='max_length', max_length=max_length)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),labels_train))\n",
    "train_dataset = train_dataset.shuffle(len(text_train)).batch(batchSize)\n",
    "\n",
    "val_encodings = tokenizer(text_val, truncation=True, padding='max_length', max_length=max_length)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), labels_val))\n",
    "val_dataset = val_dataset.shuffle(len(text_val)).batch(batchSize)\n",
    "\n",
    "test_encodings = val_encodings = tokenizer(text_test, truncation=True, padding='max_length', max_length=max_length)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), labels_test))\n",
    "test_dataset = test_dataset.shuffle(len(text_test)).batch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {}\n",
    "total_samples = len(labels_train)\n",
    "for label in np.unique(labels_train):\n",
    "    class_count = np.sum(np.array(labels_train) == label)\n",
    "    class_weights[label] = total_samples / (len(np.unique(labels_train)) * class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=4e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3,  # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data = val_dataset,\n",
    "    class_weight=class_weights,\n",
    "    steps_per_epoch = steps,\n",
    "    verbose = 1,\n",
    "    validation_steps = val_steps,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"OSITv6\"\n",
    "model.save_pretrained(out_file)\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(f'training_history_{out_file}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: don't just hardcode the subtypes\n",
    "subtypeMapping = {\n",
    "    0: \"Install or New\",\n",
    "    1: \"Service Request\",\n",
    "    2: \"Incident\",\n",
    "}\n",
    "\n",
    "def getSTId(subtype):\n",
    "    for key, value in subtypeMapping.items():\n",
    "        if value == subtype:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def getSubtype(STId):\n",
    "    for key, value in subtypeMapping.items():\n",
    "        if key == STId:\n",
    "            return value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\"I think the loss is too high here\"]\n",
    "new_encodings = tokenizer(new_text, truncation=False, padding=True)\n",
    "new_dataset = tf.data.Dataset.from_tensor_slices((dict(new_encodings))).batch(2)\n",
    "prediction = model.predict(new_dataset)\n",
    "predicted_labels = np.argmax(prediction[0])\n",
    "print(getSubtype(predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
